\documentclass[a4paper,12pt]{article}

% Packages
\usepackage{amsmath, amssymb, amsthm}
\usepackage{graphicx}
\usepackage{hyperref}
\usepackage{enumitem}
\usepackage{xcolor}
\usepackage{tcolorbox}
\usepackage{booktabs}
\usepackage{caption}
\usepackage{geometry}
\geometry{margin=1in}

% Custom Theorem Styles
\newtheorem{theorem}{Theorem}
\newtheorem{lemma}{Lemma}
\newtheorem{corollary}{Corollary}
\newtheorem{definition}{Definition}
\newtheorem{example}{Example}

% Custom Commands
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\norm}[1]{\left\lVert #1 \right\rVert}

% Title Information
\title{\textbf{Deep Learning Notes}}
\author{Amir Nourinia}
\date{\today}

\begin{document}

\maketitle

\begin{abstract}
  These notes provide an introduction to neural networks, including theoretical foundations and practical implementation details.
\end{abstract}

\tableofcontents
\newpage

\section{Introduction}
This course introduces neural networks with a hands-on approach. Each day consists of two lectures,
followed by a practical session to reinforce the material covered.

\section{Neural Networks}

\subsection{Perceptron}
A perceptron is a simple linear classifier that maps inputs to outputs.
To increase its capabilities, we introduce a nonlinear function that acts on the perceptron's output.
These nonlinear functions, called \textbf{activation functions}, help introduce complexity and allow neural
networks to model more sophisticated patterns.

\subsection{Activation Functions}
Some common activation functions are:
\begin{itemize}
  \item \textbf{Sigmoid}: $\sigma(x) = \frac{1}{1+e^{-x}}$
  \item \textbf{ReLU}: $ReLU(x) = \max(0, x)$
  \item \textbf{Softmax}: $\sigma(z_i) = \frac{e^{z_i}}{\sum_{j} e^{z_j}}$
\end{itemize}
A good activation function should be:
\begin{itemize}
  \item Efficient to compute
  \item Easy to differentiate
  \item Numerically stable
\end{itemize}

\section{Training Neural Networks}

\subsection{Loss Functions}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Loss Function]
  The loss function assigns a scalar loss value based on the current weights of the network.
\end{tcolorbox}

\subsection{Optimization Algorithms}
Optimization algorithms find optimal weights for a neural network by exploring the loss
landscape generated by the training data.

\subsubsection{Gradient Descent}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Gradient Descent]
  Gradient descent is an optimization algorithm used to minimize a function by iteratively moving in the direction of
  the negative gradient.
\end{tcolorbox}

\subsubsection{Stochastic Gradient Descent (SGD)}
\begin{tcolorbox}[colback=blue!5!white,colframe=blue!75!black,title=Stochastic Gradient Descent]
  SGD is a variant of gradient descent that updates weights using a randomly selected subset (mini-batch) of
  training data, reducing computational cost.
\end{tcolorbox}

\subsection{Backpropagation}
Backpropagation computes the gradient of the loss function with respect to each weight by propagating 
errors backward from the output layer to the input layer.

\subsection{Learning Rate}
The learning rate controls the step size of weight updates. Choosing an appropriate learning rate is 
crucial to ensure convergence and avoid divergence.

\section{References}
\begin{enumerate}
  \item Goodfellow, I., Bengio, Y., \& Courville, A. (2016). \textit{Deep Learning}. MIT Press.
  \item LeCun, Y., Bengio, Y., \& Hinton, G. (2015). Deep learning. \textit{Nature}, 521(7553), 436-444.
  \item Bishop, C. M. (2006). \textit{Pattern Recognition and Machine Learning}. Springer.
\end{enumerate}

\end{document}
